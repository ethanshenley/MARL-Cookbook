{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Vehicle Coordination: MADDPG for Traffic Management\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll implement a Multi-Agent Reinforcement Learning (MARL) solution for autonomous vehicle coordination in an urban traffic scenario. We'll use the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm, which is well-suited for continuous action spaces and can handle the complex interactions between multiple vehicles.\n",
    "\n",
    "Our scenario will consist of multiple autonomous vehicles navigating through a simplified urban environment, with the goals of reaching their destinations efficiently while avoiding collisions.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment. We'll create a simple 2D grid-based urban environment with roads, intersections, and destination points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class UrbanEnvironment:\n",
    "    def __init__(self, size: int, n_vehicles: int, n_destinations: int):\n",
    "        self.size = size\n",
    "        self.n_vehicles = n_vehicles\n",
    "        self.n_destinations = n_destinations\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.vehicle_positions = np.random.rand(self.n_vehicles, 2) * self.size\n",
    "        self.vehicle_velocities = np.zeros((self.n_vehicles, 2))\n",
    "        self.destinations = np.random.rand(self.n_destinations, 2) * self.size\n",
    "        self.collisions = np.zeros(self.n_vehicles, dtype=bool)\n",
    "        return self._get_observations()\n",
    "\n",
    "    def step(self, actions: np.ndarray) -> Tuple[np.ndarray, np.ndarray, bool, dict]:\n",
    "        # Update velocities and positions\n",
    "        self.vehicle_velocities += actions\n",
    "        self.vehicle_velocities = np.clip(self.vehicle_velocities, -1, 1)\n",
    "        self.vehicle_positions += self.vehicle_velocities\n",
    "\n",
    "        # Wrap around environment edges\n",
    "        self.vehicle_positions %= self.size\n",
    "\n",
    "        # Check for collisions\n",
    "        self.collisions = self._check_collisions()\n",
    "\n",
    "        # Calculate rewards\n",
    "        rewards = self._calculate_rewards()\n",
    "\n",
    "        # Check if all vehicles have reached destinations\n",
    "        done = np.all([self._vehicle_at_destination(i) for i in range(self.n_vehicles)])\n",
    "\n",
    "        return self._get_observations(), rewards, done, {}\n",
    "\n",
    "    def _get_observations(self) -> np.ndarray:\n",
    "        obs = np.zeros((self.n_vehicles, 4 + 2 * (self.n_vehicles - 1) + 2 * self.n_destinations))\n",
    "        for i in range(self.n_vehicles):\n",
    "            obs[i, :2] = self.vehicle_positions[i]\n",
    "            obs[i, 2:4] = self.vehicle_velocities[i]\n",
    "            other_vehicles = np.delete(self.vehicle_positions, i, axis=0)\n",
    "            obs[i, 4:4+2*(self.n_vehicles-1)] = other_vehicles.flatten()\n",
    "            obs[i, -2*self.n_destinations:] = self.destinations.flatten()\n",
    "        return obs\n",
    "\n",
    "    def _check_collisions(self) -> np.ndarray:\n",
    "        collisions = np.zeros(self.n_vehicles, dtype=bool)\n",
    "        for i in range(self.n_vehicles):\n",
    "            for j in range(i + 1, self.n_vehicles):\n",
    "                if np.linalg.norm(self.vehicle_positions[i] - self.vehicle_positions[j]) < 0.5:\n",
    "                    collisions[i] = collisions[j] = True\n",
    "        return collisions\n",
    "\n",
    "    def _calculate_rewards(self) -> np.ndarray:\n",
    "        rewards = np.zeros(self.n_vehicles)\n",
    "        for i in range(self.n_vehicles):\n",
    "            if self.collisions[i]:\n",
    "                rewards[i] -= 10\n",
    "            elif self._vehicle_at_destination(i):\n",
    "                rewards[i] += 20\n",
    "            else:\n",
    "                closest_dest = np.argmin(np.linalg.norm(self.vehicle_positions[i] - self.destinations, axis=1))\n",
    "                distance = np.linalg.norm(self.vehicle_positions[i] - self.destinations[closest_dest])\n",
    "                rewards[i] -= distance / self.size\n",
    "        return rewards\n",
    "\n",
    "    def _vehicle_at_destination(self, vehicle_idx: int) -> bool:\n",
    "        return np.any(np.linalg.norm(self.vehicle_positions[vehicle_idx] - self.destinations, axis=1) < 0.5)\n",
    "\n",
    "    def render(self):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.xlim(0, self.size)\n",
    "        plt.ylim(0, self.size)\n",
    "        plt.scatter(self.destinations[:, 0], self.destinations[:, 1], c='g', s=100, label='Destinations')\n",
    "        plt.scatter(self.vehicle_positions[:, 0], self.vehicle_positions[:, 1], c='b', s=100, label='Vehicles')\n",
    "        for i, pos in enumerate(self.vehicle_positions):\n",
    "            plt.arrow(pos[0], pos[1], self.vehicle_velocities[i, 0], self.vehicle_velocities[i, 1], \n",
    "                      head_width=0.3, head_length=0.3, fc='b', ec='b')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Test the environment\n",
    "env = UrbanEnvironment(size=10, n_vehicles=3, n_destinations=3)\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Implementation\n",
    "\n",
    "Now that we have our environment, let's implement the MADDPG algorithm. We'll create actor and critic networks, as well as the MADDPG agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_agents):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim * n_agents + action_dim * n_agents, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        x = torch.cat([states, actions], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class MADDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, n_agents):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.n_agents = n_agents\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim, n_agents)\n",
    "        self.target_actor = Actor(state_dim, action_dim)\n",
    "        self.target_critic = Critic(state_dim, action_dim, n_agents)\n",
    "        \n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.01)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.01)\n",
    "        \n",
    "        self.memory = []\n",
    "        self.max_memory_size = 10000\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.95\n",
    "        self.tau = 0.02\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = self.actor(state).detach().numpy()\n",
    "        return action\n",
    "    \n",
    "    def update(self, states, actions, rewards, next_states, dones):\n",
    "        self.memory.append((states, actions, rewards, next_states, dones))\n",
    "        if len(self.memory) > self.max_memory_size:\n",
    "            self.memory.pop(0)\n",
    "        \n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        \n",
    "        # Reshape states and actions for the critic input\n",
    "        states_full = states.view(self.batch_size, -1)\n",
    "        actions_full = actions.view(self.batch_size, -1)\n",
    "        next_states_full = next_states.view(self.batch_size, -1)\n",
    "        \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions = torch.cat([self.target_actor(next_states[:, i]) \n",
    "                                      for i in range(self.n_agents)], dim=1)\n",
    "            Q_targets_next = self.target_critic(next_states_full, next_actions)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic(states_full, actions_full)\n",
    "        critic_loss = nn.MSELoss()(Q_expected, Q_targets.detach())\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        actor_actions = torch.cat([self.actor(states[:, i]) \n",
    "                                   for i in range(self.n_agents)], dim=1)\n",
    "        actor_loss = -self.critic(states_full, actor_actions).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "# Initialize environment and agents\n",
    "env = UrbanEnvironment(size=10, n_vehicles=3, n_destinations=3)\n",
    "state_dim = 4 + 2 * (env.n_vehicles - 1) + 2 * env.n_destinations\n",
    "action_dim = 2\n",
    "agents = [MADDPGAgent(state_dim, action_dim, env.n_vehicles) for _ in range(env.n_vehicles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now that we have our environment and MADDPG agents set up, let's implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 14 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(rewards)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mMADDPGAgent.update\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Update critic\u001b[39;00m\n\u001b[1;32m     72\u001b[0m next_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor(next_states[:, i\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim]) \n\u001b[1;32m     73\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m Q_targets_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m Q_targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m Q_targets_next \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones))\n\u001b[1;32m     76\u001b[0m Q_expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(states, actions)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, states, actions)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, actions):\n\u001b[0;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 14 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "max_steps = 100\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        actions = np.array([agent.get_action(s) for agent, s in zip(agents, state)])\n",
    "        \n",
    "        next_state, rewards, done, _ = env.step(actions)\n",
    "        episode_reward += sum(rewards)\n",
    "        \n",
    "        # Store the experience in each agent's memory\n",
    "        for agent in agents:\n",
    "            agent.update(state, actions, rewards, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Avg Reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's evaluate our trained agents by running a few episodes and visualizing their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agents, n_episodes=5):\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step < 100:\n",
    "            env.render()\n",
    "            actions = [agent.get_action(s) for agent, s in zip(agents, state)]\n",
    "            actions = np.array(actions)\n",
    "            state, rewards, done, _ = env.step(actions)\n",
    "            step += 1\n",
    "        \n",
    "        print(f\"Episode {episode + 1} finished in {step} steps\")\n",
    "\n",
    "evaluate(env, agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented an Autonomous Vehicle Coordination scenario using the MADDPG algorithm. We've seen how multiple agents can learn to navigate an urban environment, avoiding collisions and reaching their destinations.\n",
    "\n",
    "Some observations and potential improvements:\n",
    "\n",
    "1. The agents learn to navigate the environment and avoid collisions, but their strategies might not be optimal.\n",
    "2. We could improve the realism of the environment by adding more complex road structures, traffic rules, or dynamic obstacles.\n",
    "3. The reward function could be refined to encourage more efficient routes or to penalize unnecessary movements.\n",
    "4. We could experiment with different network architectures or hyperparameters to improve learning performance.\n",
    "5. Implementing prioritized experience replay or parameter noise for exploration could potentially enhance the learning process.\n",
    "\n",
    "In the next notebooks, we'll explore even more complex scenarios and advanced MARL techniques to address some of these limitations and tackle different real-world problems.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., & Mordatch, I. (2017). Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (pp. 6379-6390).\n",
    "2. Shalev-Shwartz, S., Shammah, S., & Shashua, A. (2016). Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
