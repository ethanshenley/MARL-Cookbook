{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predator-Prey Pursuit: A Multi-Agent Reinforcement Learning Approach\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll implement a classic Multi-Agent Reinforcement Learning (MARL) scenario: Predator-Prey Pursuit. This problem involves both cooperative and competitive elements, making it an intuitive starting point for exploring MARL techniques.\n",
    "\n",
    "Our scenario will consist of multiple predator agents working together to capture prey agents in a grid world environment. We'll use Independent Q-Learning with parameter sharing as our MARL approach.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment. We'll use a simple grid world where predators and prey can move in four directions: up, down, left, and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "class PredatorPreyEnv:\n",
    "    def __init__(self, grid_size: int, n_predators: int, n_prey: int):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_predators = n_predators\n",
    "        self.n_prey = n_prey\n",
    "        self.initial_n_prey = n_prey  # Store the initial number of prey\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.predator_positions = np.random.randint(0, self.grid_size, size=(self.n_predators, 2))\n",
    "        self.prey_positions = np.random.randint(0, self.grid_size, size=(self.initial_n_prey, 2))\n",
    "        return self._get_observations()\n",
    "\n",
    "    def step(self, predator_actions: List[int]) -> Tuple[np.ndarray, List[float], bool, dict]:\n",
    "        # Move predators\n",
    "        for i, action in enumerate(predator_actions):\n",
    "            self._move_agent(self.predator_positions[i], action)\n",
    "\n",
    "        # Move prey (random movement)\n",
    "        for i in range(len(self.prey_positions)):\n",
    "            self._move_agent(self.prey_positions[i], np.random.randint(0, 4))\n",
    "\n",
    "        # Check for captures\n",
    "        rewards = [0] * self.n_predators\n",
    "        captured_prey = []\n",
    "        for i, prey_pos in enumerate(self.prey_positions):\n",
    "            if any(np.all(prey_pos == pred_pos) for pred_pos in self.predator_positions):\n",
    "                rewards = [1] * self.n_predators  # All predators get reward for capture\n",
    "                captured_prey.append(i)\n",
    "\n",
    "        # Remove captured prey\n",
    "        self.prey_positions = np.delete(self.prey_positions, captured_prey, axis=0)\n",
    "\n",
    "        done = len(self.prey_positions) == 0\n",
    "        return self._get_observations(), rewards, done, {}\n",
    "\n",
    "    def _move_agent(self, position: np.ndarray, action: int):\n",
    "        if action == 0:  # Up\n",
    "            position[0] = max(0, position[0] - 1)\n",
    "        elif action == 1:  # Down\n",
    "            position[0] = min(self.grid_size - 1, position[0] + 1)\n",
    "        elif action == 2:  # Left\n",
    "            position[1] = max(0, position[1] - 1)\n",
    "        elif action == 3:  # Right\n",
    "            position[1] = min(self.grid_size - 1, position[1] + 1)\n",
    "\n",
    "    def _get_observations(self) -> np.ndarray:\n",
    "        obs = np.zeros((self.n_predators, 2 * (self.n_predators + self.initial_n_prey)))\n",
    "        for i in range(self.n_predators):\n",
    "            obs[i, :2*self.n_predators] = self.predator_positions.flatten()\n",
    "            prey_obs = self.prey_positions.flatten()\n",
    "            obs[i, 2*self.n_predators:2*(self.n_predators + len(self.prey_positions))] = prey_obs\n",
    "            # The remaining elements are already zero-padded\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size, 3))\n",
    "        for pos in self.predator_positions:\n",
    "            grid[pos[0], pos[1]] = [1, 0, 0]  # Red for predators\n",
    "        for pos in self.prey_positions:\n",
    "            grid[pos[0], pos[1]] = [0, 1, 0]  # Green for prey\n",
    "        plt.imshow(grid)\n",
    "        plt.show()\n",
    "\n",
    "# Test the environment\n",
    "env = PredatorPreyEnv(grid_size=10, n_predators=3, n_prey=2)\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation\n",
    "\n",
    "Now that we have our environment, let's implement our predator agents using Independent Q-Learning with parameter sharing. We'll use a simple neural network to approximate the Q-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class PredatorAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters())\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.update_target_every = 100\n",
    "        self.update_counter = 0\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 3)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(torch.FloatTensor(state))\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.update_target_every == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# Initialize environment and agents\n",
    "env = PredatorPreyEnv(grid_size=10, n_predators=3, n_prey=2)\n",
    "state_dim = 2 * (env.n_predators + env.n_prey)\n",
    "action_dim = 4\n",
    "agents = [PredatorAgent(state_dim, action_dim) for _ in range(env.n_predators)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now that we have our environment and agents set up, let's implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "max_steps = 100\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        actions = [agent.get_action(s) for agent, s in zip(agents, state)]\n",
    "        next_state, rewards, done, _ = env.step(actions)\n",
    "        episode_reward += sum(rewards)\n",
    "        \n",
    "        for i, agent in enumerate(agents):\n",
    "            agent.update(state[i], actions[i], rewards[i], next_state[i], done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Avg Reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's evaluate our trained agents by running a few episodes and visualizing their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agents, n_episodes=5):\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step < 100:\n",
    "            env.render()\n",
    "            actions = [agent.get_action(s) for agent, s in zip(agents, state)]\n",
    "            state, _, done, _ = env.step(actions)\n",
    "            step += 1\n",
    "        \n",
    "        print(f\"Episode {episode + 1} finished in {step} steps\")\n",
    "\n",
    "evaluate(env, agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a basic Predator-Prey Pursuit scenario using Independent Q-Learning with parameter sharing. We've seen how multiple agents can learn to cooperate to capture prey in a grid world environment.\n",
    "\n",
    "Some observations and potential improvements:\n",
    "\n",
    "1. The agents learn to pursue and capture prey, but their strategy might not be optimal.\n",
    "2. We could improve coordination by implementing more advanced MARL techniques like QMIX or MADDPG.\n",
    "3. The environment is relatively simple. We could increase complexity by adding obstacles or giving prey more sophisticated evasion strategies.\n",
    "4. We're using a simple feedforward neural network. Recurrent neural networks (RNNs) might perform better by allowing agents to remember past observations.\n",
    "\n",
    "In the next notebooks, we'll explore more complex scenarios and advanced MARL techniques to address some of these limitations.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning (pp. 330-337).\n",
    "2. Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., ... & Vicente, R. (2017). Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4), e0172395."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
