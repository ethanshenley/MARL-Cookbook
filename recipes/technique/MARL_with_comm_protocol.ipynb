{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARL with Communication Protocols\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore how to implement communication protocols in Multi-Agent Reinforcement Learning (MARL) systems. Effective communication between agents can significantly improve their ability to collaborate and solve complex tasks. In the context of app modernization, this could represent different components of the modernization process sharing information and coordinating their actions.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Agents with Communication\n",
    "\n",
    "We'll create a simple environment where agents need to communicate to solve a task. Our agents will use a differentiable communication channel to share information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_agents = 3\n",
    "state_dim = 10\n",
    "hidden_dim = 64\n",
    "action_dim = 5\n",
    "message_dim = 8\n",
    "\n",
    "class CommunicatingAgent(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, message_dim, num_agents):\n",
    "        super(CommunicatingAgent, self).__init__()\n",
    "        input_dim = state_dim + (num_agents - 1) * message_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.action_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.message_head = nn.Linear(hidden_dim, message_dim)\n",
    "        \n",
    "    def forward(self, state, messages):\n",
    "        # Flatten received messages and concatenate with state\n",
    "        x = torch.cat([state, messages.flatten(1)], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action = self.action_head(x)\n",
    "        new_message = self.message_head(x)\n",
    "        return action, new_message\n",
    "\n",
    "class CommunicationChannel:\n",
    "    def __init__(self, num_agents, message_dim):\n",
    "        self.num_agents = num_agents\n",
    "        self.message_dim = message_dim\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.messages = torch.zeros(self.num_agents, self.message_dim).detach()\n",
    "\n",
    "    def send(self, agent_id, message):\n",
    "        # Clone the message to avoid in-place modifications\n",
    "        self.messages[agent_id] = message.clone().detach()\n",
    "\n",
    "    def receive(self, agent_id):\n",
    "        # Return messages from other agents, avoiding in-place modifications\n",
    "        return torch.cat([self.messages[:agent_id], self.messages[agent_id+1:]], dim=0).detach()\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, num_agents, state_dim, action_dim):\n",
    "        self.num_agents = num_agents\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def reset(self):\n",
    "        return torch.randn(self.num_agents, self.state_dim)\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Simplified environment dynamics\n",
    "        next_states = torch.randn(self.num_agents, self.state_dim)\n",
    "        rewards = torch.sum(actions, dim=-1)  # Reward is sum of action values\n",
    "        done = False\n",
    "        return next_states, rewards, done\n",
    "\n",
    "# Initialize agents and environment\n",
    "agents = [CommunicatingAgent(state_dim, hidden_dim, action_dim, message_dim, num_agents) for _ in range(num_agents)]\n",
    "comm_channel = CommunicationChannel(num_agents, message_dim)\n",
    "env = Environment(num_agents, state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now, let's implement a training loop where agents learn to communicate and collaborate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agents(num_episodes, max_steps):\n",
    "    optimizers = [optim.Adam(agent.parameters(), lr=0.001) for agent in agents]\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        comm_channel.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            messages = []\n",
    "\n",
    "            for i, agent in enumerate(agents):\n",
    "                received_messages = comm_channel.receive(i).clone()  # Ensure no in-place modification\n",
    "                action, message = agent(states[i].unsqueeze(0), received_messages.unsqueeze(0))\n",
    "                actions.append(action.squeeze(0).clone())  # Clone to avoid in-place modifications\n",
    "                messages.append(message.squeeze(0).clone())  # Clone to avoid in-place modifications\n",
    "                comm_channel.send(i, message.clone())  # Clone message before sending\n",
    "\n",
    "            actions = torch.stack(actions)  # Ensure no in-place modification here\n",
    "            next_states, rewards, done = env.step(actions)\n",
    "            episode_reward += rewards.sum().item()\n",
    "\n",
    "            # Compute losses based on actions\n",
    "            for i, agent in enumerate(agents):\n",
    "                loss = (actions[i] ** 2).sum()  # Loss based on actions\n",
    "                optimizers[i].zero_grad()\n",
    "                \n",
    "                retain_graph_flag = i < len(agents) - 1\n",
    "                loss.backward(retain_graph=retain_graph_flag)\n",
    "                optimizers[i].step()\n",
    "\n",
    "            states = next_states.detach()  # Detach next_states to avoid graph retention\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Avg Reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "# Train the agents\n",
    "num_episodes = 1000\n",
    "max_steps = 50\n",
    "rewards = train_agents(num_episodes, max_steps)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Communication\n",
    "\n",
    "Let's analyze the learned communication protocols by visualizing the messages sent between agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_messages(num_steps=10):\n",
    "    states = env.reset()\n",
    "    comm_channel.reset()\n",
    "    message_history = []\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        step_messages = []\n",
    "        for i, agent in enumerate(agents):\n",
    "            received_messages = comm_channel.receive(i)\n",
    "            _, message = agent(states[i], received_messages)\n",
    "            step_messages.append(message.detach().numpy())\n",
    "            comm_channel.send(i, message)\n",
    "        message_history.append(step_messages)\n",
    "\n",
    "    message_history = np.array(message_history)\n",
    "\n",
    "    fig, axes = plt.subplots(num_agents, 1, figsize=(10, 5*num_agents))\n",
    "    for i in range(num_agents):\n",
    "        im = axes[i].imshow(message_history[:, i, :].T, aspect='auto', cmap='viridis')\n",
    "        axes[i].set_title(f\"Agent {i+1} Messages\")\n",
    "        axes[i].set_xlabel(\"Time Step\")\n",
    "        axes[i].set_ylabel(\"Message Dimension\")\n",
    "        plt.colorbar(im, ax=axes[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented a MARL system with a communication protocol. We saw how agents can learn to communicate and collaborate to solve a task. This approach has several potential applications in app modernization:\n",
    "\n",
    "1. Coordinating different aspects of the modernization process (e.g., code refactoring, database migration, and infrastructure updates)\n",
    "2. Sharing information about dependencies and potential conflicts during the modernization process\n",
    "3. Collaborative decision-making for architectural choices in the modernized application\n",
    "\n",
    "Future work could involve:\n",
    "- Implementing more sophisticated communication protocols (e.g., attention mechanisms)\n",
    "- Applying this approach to specific app modernization tasks\n",
    "- Analyzing the emergent communication patterns in the context of software engineering practices\n",
    "\n",
    "## References\n",
    "\n",
    "1. Foerster, J., et al. (2016). Learning to communicate with deep multi-agent reinforcement learning. NeurIPS.\n",
    "2. Sukhbaatar, S., Fergus, R., et al. (2016). Learning multiagent communication with backpropagation. NeurIPS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
