{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial MARL for Robustness\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore Adversarial Multi-Agent Reinforcement Learning (MARL) to improve the robustness of our agents. This approach is particularly useful in scenarios where agents need to perform well in uncertain or potentially hostile environments. In the context of app modernization, this could represent dealing with legacy code vulnerabilities, unexpected system behaviors, or potential security threats during the modernization process.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Adversarial Agents\n",
    "\n",
    "We'll create a simple environment where we have two types of agents: protagonists and adversaries. The protagonists will try to accomplish a task, while the adversaries will try to hinder their progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Agent, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class AdversarialEnvironment:\n",
    "    def __init__(self, num_protagonists, num_adversaries, state_dim, action_dim):\n",
    "        self.num_protagonists = num_protagonists\n",
    "        self.num_adversaries = num_adversaries\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def reset(self):\n",
    "        return torch.randn(self.num_protagonists + self.num_adversaries, self.state_dim)\n",
    "    \n",
    "    def step(self, protagonist_actions, adversary_actions):\n",
    "        # Simplified environment dynamics\n",
    "        protagonist_effect = torch.sum(protagonist_actions, dim=0)\n",
    "        adversary_effect = torch.sum(adversary_actions, dim=0)\n",
    "        \n",
    "        next_states = torch.randn(self.num_protagonists + self.num_adversaries, self.state_dim)\n",
    "        \n",
    "        protagonist_rewards = torch.sum(protagonist_effect - adversary_effect, dim=-1).repeat(self.num_protagonists)\n",
    "        adversary_rewards = -protagonist_rewards[:self.num_adversaries]\n",
    "        \n",
    "        done = False\n",
    "        return next_states, protagonist_rewards, adversary_rewards, done\n",
    "\n",
    "# Hyperparameters\n",
    "num_protagonists = 2\n",
    "num_adversaries = 1\n",
    "state_dim = 10\n",
    "hidden_dim = 64\n",
    "action_dim = 5\n",
    "\n",
    "# Initialize agents and environment\n",
    "protagonists = [Agent(state_dim, hidden_dim, action_dim) for _ in range(num_protagonists)]\n",
    "adversaries = [Agent(state_dim, hidden_dim, action_dim) for _ in range(num_adversaries)]\n",
    "env = AdversarialEnvironment(num_protagonists, num_adversaries, state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now, let's implement a training loop where protagonists learn to accomplish their task while adapting to adversarial actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     48\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 49\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Plot the learning curve\u001b[39;00m\n\u001b[1;32m     52\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m, in \u001b[0;36mtrain_agents\u001b[0;34m(num_episodes, max_steps)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m optimizer \u001b[38;5;129;01min\u001b[39;00m adversary_optimizers:\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtotal_adversary_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m optimizer \u001b[38;5;129;01min\u001b[39;00m adversary_optimizers:\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "def train_agents(num_episodes, max_steps):\n",
    "    protagonist_optimizers = [optim.Adam(agent.parameters(), lr=0.001) for agent in protagonists]\n",
    "    adversary_optimizers = [optim.Adam(agent.parameters(), lr=0.001) for agent in adversaries]\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            protagonist_actions = torch.stack([agent(states[i].unsqueeze(0)).squeeze(0) \n",
    "                                               for i, agent in enumerate(protagonists)])\n",
    "            adversary_actions = torch.stack([agent(states[i+num_protagonists].unsqueeze(0)).squeeze(0) \n",
    "                                             for i, agent in enumerate(adversaries)])\n",
    "\n",
    "            next_states, protagonist_rewards, adversary_rewards, done = env.step(protagonist_actions, adversary_actions)\n",
    "            episode_reward += protagonist_rewards.sum().item()\n",
    "\n",
    "            # Combine losses\n",
    "            total_loss = sum(-protagonist_rewards[i] for i in range(len(protagonists))) + \\\n",
    "                         sum(-adversary_rewards[i] for i in range(len(adversaries)))\n",
    "\n",
    "            # Zero gradients for all optimizers\n",
    "            for optimizer in protagonist_optimizers + adversary_optimizers:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Single backward pass\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Step optimizers\n",
    "            for optimizer in protagonist_optimizers + adversary_optimizers:\n",
    "                optimizer.step()\n",
    "\n",
    "            states = next_states\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Avg Reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "\n",
    "# Train the agents\n",
    "num_episodes = 1000\n",
    "max_steps = 50\n",
    "rewards = train_agents(num_episodes, max_steps)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Learning Curve (Protagonist Rewards)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Robustness\n",
    "\n",
    "Let's evaluate the robustness of our trained protagonists by testing them against different adversarial strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(num_evaluations, max_steps):\n",
    "    evaluation_rewards = []\n",
    "\n",
    "    for _ in range(num_evaluations):\n",
    "        states = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            protagonist_actions = torch.stack([agent(states[i].unsqueeze(0)).squeeze(0) \n",
    "                                               for i, agent in enumerate(protagonists)])\n",
    "            \n",
    "            # Use random actions for adversaries to test robustness\n",
    "            adversary_actions = torch.randn(num_adversaries, action_dim)\n",
    "\n",
    "            next_states, protagonist_rewards, _, done = env.step(protagonist_actions, adversary_actions)\n",
    "            episode_reward += protagonist_rewards.sum().item()\n",
    "\n",
    "            states = next_states\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        evaluation_rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(evaluation_rewards), np.std(evaluation_rewards)\n",
    "\n",
    "mean_reward, std_reward = evaluate_robustness(100, max_steps)\n",
    "print(f\"Robustness Evaluation - Mean Reward: {mean_reward:.2f}, Std Reward: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented an Adversarial MARL system to improve the robustness of our agents. We saw how protagonists can learn to perform their task while adapting to adversarial actions. This approach has several potential applications in app modernization:\n",
    "\n",
    "1. Improving the resilience of modernized applications against potential security threats\n",
    "2. Developing robust migration strategies that can handle unexpected issues during the modernization process\n",
    "3. Testing the reliability of modernized systems under various adverse conditions\n",
    "\n",
    "Future work could involve:\n",
    "- Implementing more sophisticated adversarial strategies\n",
    "- Applying this approach to specific app modernization challenges, such as handling legacy code vulnerabilities\n",
    "- Exploring the trade-offs between performance and robustness in the context of app modernization\n",
    "\n",
    "## References\n",
    "\n",
    "1. Pinto, L., Davidson, J., Sukthankar, R., & Gupta, A. (2017). Robust adversarial reinforcement learning. ICML.\n",
    "2. Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S., & Russell, S. (2020). Adversarial policies: Attacking deep reinforcement learning. ICLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
