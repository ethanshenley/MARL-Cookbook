{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARL for Distributed Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore how Multi-Agent Reinforcement Learning (MARL) can be applied to distributed optimization problems. This approach is particularly relevant for app modernization scenarios where different components of a system need to be optimized simultaneously, taking into account their interdependencies.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x79d46bf5bbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Distributed Optimization Agents\n",
    "\n",
    "We'll create a simple environment where multiple agents need to collaborate to optimize a global objective function. Each agent will be responsible for optimizing a subset of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationAgent(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(OptimizationAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DistributedOptimizationEnvironment:\n",
    "    def __init__(self, num_agents, param_dim, target_value):\n",
    "        self.num_agents = num_agents\n",
    "        self.param_dim = param_dim\n",
    "        self.target_value = target_value\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.global_params = torch.rand(self.param_dim) * 2 - 1  # Initialize between -1 and 1\n",
    "        return self.global_params.repeat(self.num_agents, 1)\n",
    "    \n",
    "    def step(self, actions):\n",
    "        # Initialize a new global_params tensor\n",
    "        new_global_params = self.global_params.clone()\n",
    "        for i, action in enumerate(actions):\n",
    "            start_idx = i * (self.param_dim // self.num_agents)\n",
    "            end_idx = (i + 1) * (self.param_dim // self.num_agents)\n",
    "            # Update without in-place operation\n",
    "            new_global_params[start_idx:end_idx] = new_global_params[start_idx:end_idx] + action\n",
    "        \n",
    "        # Clip parameters to be between -1 and 1\n",
    "        new_global_params = torch.clamp(new_global_params, -1, 1)\n",
    "        \n",
    "        # Calculate reward based on distance to target value\n",
    "        reward = -torch.abs(self.target_value - torch.sum(new_global_params))\n",
    "        done = torch.abs(self.target_value - torch.sum(new_global_params)) < 0.1\n",
    "        \n",
    "        # Update the global parameters\n",
    "        self.global_params = new_global_params.detach()\n",
    "        \n",
    "        return self.global_params.repeat(self.num_agents, 1), reward.repeat(self.num_agents), done\n",
    "\n",
    "# Hyperparameters\n",
    "num_agents = 4\n",
    "param_dim = 20\n",
    "hidden_dim = 64\n",
    "target_value = 5.0\n",
    "\n",
    "# Initialize agents and environment\n",
    "agents = [OptimizationAgent(param_dim, hidden_dim, param_dim // num_agents) for _ in range(num_agents)]\n",
    "env = DistributedOptimizationEnvironment(num_agents, param_dim, target_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now, let's implement a training loop where agents learn to collaboratively optimize the global objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/torch/autograd/graph.py:769: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1245357/1412191364.py\", line 34, in <module>\n",
      "    rewards = train_agents(num_episodes, max_steps)\n",
      "  File \"/tmp/ipykernel_1245357/1412191364.py\", line 10, in train_agents\n",
      "    actions = [agent(states[i].unsqueeze(0)).squeeze(0) for i, agent in enumerate(agents)]\n",
      "  File \"/tmp/ipykernel_1245357/1412191364.py\", line 10, in <listcomp>\n",
      "    actions = [agent(states[i].unsqueeze(0)).squeeze(0) for i, agent in enumerate(agents)]\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1245357/3463533033.py\", line 11, in forward\n",
      "    return self.fc3(x)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ethan/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:111.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 5]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     33\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 34\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Plot the learning curve\u001b[39;00m\n\u001b[1;32m     37\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mtrain_agents\u001b[0;34m(num_episodes, max_steps)\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mrewards[i]\n\u001b[1;32m     17\u001b[0m     optimizers[i]\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     optimizers[i]\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m states \u001b[38;5;241m=\u001b[39m next_states\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 5]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "def train_agents(num_episodes, max_steps):\n",
    "    optimizers = [optim.Adam(agent.parameters(), lr=0.001) for agent in agents]\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            actions = [agent(states[i].unsqueeze(0)).squeeze(0) for i, agent in enumerate(agents)]\n",
    "            next_states, rewards, done = env.step(actions)\n",
    "            episode_reward += rewards[0].item()\n",
    "\n",
    "            # Aggregate losses\n",
    "            total_loss = 0\n",
    "            for i, agent in enumerate(agents):\n",
    "                total_loss += -rewards[i]\n",
    "\n",
    "            # Update agents collectively\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.step()\n",
    "\n",
    "            states = next_states  # Remove .detach()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Avg Reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Train the agents\n",
    "num_episodes = 1000\n",
    "max_steps = 50\n",
    "rewards = train_agents(num_episodes, max_steps)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Optimized Solution\n",
    "\n",
    "Let's evaluate the final solution found by our agents and visualize how close it is to the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_solution():\n",
    "    states = env.reset()\n",
    "    actions = [agent(states[i].unsqueeze(0)).squeeze(0) for i, agent in enumerate(agents)]\n",
    "    final_states, _, _ = env.step(actions)\n",
    "    \n",
    "    final_value = torch.sum(env.global_params).item()\n",
    "    \n",
    "    print(f\"Target value: {env.target_value}\")\n",
    "    print(f\"Final optimized value: {final_value}\")\n",
    "    print(f\"Difference: {abs(env.target_value - final_value)}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(param_dim), env.global_params.numpy())\n",
    "    plt.title(\"Final Parameter Values\")\n",
    "    plt.xlabel(\"Parameter Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.axhline(y=env.target_value / param_dim, color='r', linestyle='--', label='Target Average')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented a MARL approach to distributed optimization. We saw how multiple agents can learn to collaboratively optimize a global objective function by each focusing on a subset of parameters. This approach has several potential applications in app modernization:\n",
    "\n",
    "1. Optimizing multiple components of a large-scale application simultaneously\n",
    "2. Balancing resource allocation across different microservices\n",
    "3. Tuning hyperparameters of distributed machine learning models in modernized AI/ML pipelines\n",
    "\n",
    "Future work could involve:\n",
    "- Implementing more complex objective functions that better represent real-world app modernization challenges\n",
    "- Exploring different communication strategies between agents to improve coordination\n",
    "- Applying this approach to specific app modernization tasks, such as optimizing database queries or load balancing\n",
    "\n",
    "## References\n",
    "\n",
    "1. Qu, G., & Li, N. (2019). Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems, 7(1), 19-29.\n",
    "2. Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., & Whiteson, S. (2018). Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 32, No. 1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
